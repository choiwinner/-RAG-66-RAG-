{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "896f08b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#인코더"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8a0f2dd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "컨텍스트 벡터: 1.5750000000000002\n"
     ]
    }
   ],
   "source": [
    "def encode(input_sequence, embedding_matrix):\n",
    "    \"\"\"\n",
    "    입력 시퀀스를 컨텍스트 벡터로 인코딩하는 간소화된 함수입니다.\n",
    "\n",
    "    Args:\n",
    "    - input_sequence: 문장 내 단어들을 대표하는 정수의 리스트입니다.\n",
    "    - embedding_matrix: 단어 인덱스를 임베딩으로 변환하는 행렬입니다.\n",
    "\n",
    "    Returns:\n",
    "    입력 시퀀스 전체의 정보를 담은 단일 벡터를 반환합니다.\n",
    "    \"\"\"\n",
    "    # 단계 1: 단어 임베딩\n",
    "    # 입력된 각 단어를 고차원 벡터로 변환합니다.\n",
    "    embedded_sequence = [embedding_matrix[word] for word in input_sequence]\n",
    "\n",
    "    # 단계 2: Self-Attention : 단어들 사이의 상호작용을 계산합니다.\n",
    "    # 여기서는 모든 임베딩의 합을 임베딩의 수로 나누어 평균 벡터를 생성합니다.\n",
    "    attention_output = sum(embedded_sequence) / len(embedded_sequence)\n",
    "\n",
    "    # 단계 3: 포지션-와이즈 피드포워드 네트워크 :각 위치의 출력을 독립적으로 변환하는 과정을 단순화\n",
    "    # 여기서는 결과 벡터에 고정된 스칼라(1.5)를 곱합니다.\n",
    "    context_vector = attention_output * 1.5  \n",
    "\n",
    "    return context_vector\n",
    "\n",
    "# 예시 사용법:\n",
    "input_sequence = [3, 14, 15, 92]  # 단어 인덱스로 표현된 예제 문장\n",
    "embedding_matrix = {\n",
    "    3: [0.1, 0.2],\n",
    "    14: [0.4, 0.5],\n",
    "    15: [0.6, 0.7],\n",
    "    92: [0.8, 0.9]\n",
    "}  # 간소화된 임베딩 행렬\n",
    "\n",
    "context_vector = encode(input_sequence, embedding_matrix)\n",
    "print(\"컨텍스트 벡터:\", context_vector)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6179fb86",
   "metadata": {},
   "outputs": [],
   "source": [
    "#예를 들어, 문장을 입력으로 받았다면, 이 컨텍스트 벡터는 해당 문장의 전반적인 의미, 중요 단어나 구, 문맥 등을 모델이 이해한 바를 요약하여 담고 있습니다. \n",
    "#따라서, 컨텍스트 벡터는 후속 처리 과정(예: 번역, 요약, 감성 분석 등)에서 매우 중요한 역할을 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "98650a22",
   "metadata": {},
   "outputs": [],
   "source": [
    "#디코더"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "984e9ddf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Sequence: ['hello', 'end']\n"
     ]
    }
   ],
   "source": [
    "def simple_decoder(encoded_context, start_token, vocabulary, embedding_matrix, max_length):\n",
    "    \"\"\"\n",
    "    간소화된 디코더 함수. 인코딩된 컨텍스트 벡터를 사용하여 새로운 시퀀스를 생성합니다.\n",
    "    \n",
    "    Args:\n",
    "    - encoded_context: 인코더로부터 넘어온 컨텍스트 벡터.\n",
    "    - start_token: 출력 시퀀스 생성을 시작하는 토큰.\n",
    "    - vocabulary: 가능한 모든 출력 토큰의 집합.\n",
    "    - embedding_matrix: 단어 임베딩을 위한 행렬.\n",
    "    - max_length: 생성할 시퀀스의 최대 길이.\n",
    "    \n",
    "    Returns:\n",
    "    생성된 시퀀스의 단어 인덱스 리스트.\n",
    "    \"\"\"\n",
    "    output_sequence = [start_token]\n",
    "    \n",
    "    for _ in range(max_length):\n",
    "        # 현재까지의 출력 시퀀스를 기반으로 다음 토큰 예측 (단순화된 예시로, 무작위 선택을 사용)\n",
    "        next_token = random.choice(list(vocabulary))\n",
    "        output_sequence.append(next_token)\n",
    "        \n",
    "        # 'end' 토큰을 만나면 시퀀스 생성 종료\n",
    "        if next_token == 'end':\n",
    "            break\n",
    "    \n",
    "    return output_sequence\n",
    "\n",
    "# 예시 사용\n",
    "encoded_context = [0.5, -0.1]  # 인코더로부터의 컨텍스트 벡터 예시\n",
    "start_token = 'hello'  # 시퀀스 생성을 위한 시작 토큰\n",
    "vocabulary = {'start', 'hello', 'world', 'end'}  # 가능한 단어 집합\n",
    "max_length = 5  # 생성할 시퀀스의 최대 길이\n",
    "\n",
    "# 디코더 실행\n",
    "output_sequence = simple_decoder(encoded_context, start_token, vocabulary, embedding_matrix, max_length)\n",
    "print(\"Generated Sequence:\", output_sequence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2a6bfa4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openai_f",
   "language": "python",
   "name": "openai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
